{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb150042-063f-498e-a675-57b1ad020c61",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/HiddenBeginner/notebooks/blob/master/video-data-analysis/Tutorial_on_Video_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f433d08c-5fff-4738-ac97-14aeab4ad477",
   "metadata": {
    "id": "f433d08c-5fff-4738-ac97-14aeab4ad477"
   },
   "source": [
    "# Code Tutorial on Video Analysis\n",
    "\n",
    "**Video Credits & Usage**\n",
    "- **Source**: The original video used in this tutorial can be found at [https://databrary.org/volume/235](https://databrary.org/volume/235), which is publicly available.\n",
    "- **Note on Implementation:** The original video exceeds 14 minutes. Therefore, we will use the last 60 seconds of the clip for fast implementation.\n",
    "- **Availability**: ~~The `wget` code below will download the video clip that I've uploaded on Google Drive and the url will expire after this tutorial~~. Therefore, if you want to reproduce this tutorial later, please download the original video on Databrary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887c20b9-c8f0-400c-b251-5acdafdd663a",
   "metadata": {
    "id": "887c20b9-c8f0-400c-b251-5acdafdd663a"
   },
   "source": [
    "### Loading and exploring a video file using `moviepy`\n",
    "\n",
    "There are two popular packages for handling video data, `moviepy` and `opencv-python` (`cv2`).\n",
    "- `moviepy` is designed for high-level video editing. It is optimized for video-level composition and effects (e.g., fades, transitions, and overlays). However, `moviepy` is less efficient for frame-by-frame analysis and converting/writing videos.\n",
    "- `opencv-python` is optimized for low-level, frame-by-frame processing and computer vision tasks. However, it does not support audio processing and high-level video editing.\n",
    "\n",
    "In this tutorial, we will use `moviepy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aafcff5-99e8-4cbc-9595-7e4bbecaf06b",
   "metadata": {
    "id": "1aafcff5-99e8-4cbc-9595-7e4bbecaf06b"
   },
   "outputs": [],
   "source": [
    "raw_video_fpath = './0807.mp4'\n",
    "short_video_fpath = './video.mp4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52a5369-676b-46b2-91e8-16e0297104a4",
   "metadata": {
    "id": "f52a5369-676b-46b2-91e8-16e0297104a4"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from moviepy.editor import VideoFileClip, CompositeVideoClip, TextClip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee00d0d-b87d-4e77-aaae-6689e889ddd9",
   "metadata": {
    "id": "fee00d0d-b87d-4e77-aaae-6689e889ddd9"
   },
   "outputs": [],
   "source": [
    "# The annotated code below processes the raw video to a short video clip\n",
    "\n",
    "# raw_clip = VideoFileClip(raw_video_fpath)\n",
    "# # Taking the video clip from the last 60 seconds (`t_start=-60.0`) to the end of the video (`t_end`=None)\n",
    "# clip = raw_clip.subclip(t_start=-60.0, t_end=None)\n",
    "# clip.write_videofile(short_video_fpath)\n",
    "\n",
    "clip = VideoFileClip(short_video_fpath)\n",
    "\n",
    "print(\"Frame size (width, height): \", clip.size)\n",
    "print(\"Duration (sec): \", clip.duration)\n",
    "print(\"Frames per second (FPS): \", clip.fps)\n",
    "print(\"Total number of frames: \", int(clip.duration * clip.fps) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63547bb5-4344-4599-8b7b-206eec5fc08a",
   "metadata": {
    "id": "63547bb5-4344-4599-8b7b-206eec5fc08a"
   },
   "outputs": [],
   "source": [
    "clip.ipython_display(maxduration=60.1, width=360, rd_kwargs={'logger': None})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d54532c-1807-4a6d-8643-81fc831c6237",
   "metadata": {
    "id": "2d54532c-1807-4a6d-8643-81fc831c6237"
   },
   "source": [
    "- The duration and FPS in the previous code are parsed from the video file's **header**, which may occasionally deviate from the actual frames in the video stream.\n",
    "- Thus, it is recommended to store the actual timestamps, called Presentation Time Stamp (PTS), rather than relying on values calculated based on duration and FPS.\n",
    "- Frames are usually processed one by one. This is because a video file saves space by only stroing the changes between frames. Because of this 'inter-frame' compression, you can't just jump in anywhere.\n",
    "- From this reason, the reconstructed frames can occupy up to 1.74 GB memory, even though our video file has a size of 5.8 MB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e384a898-9424-4067-800b-eff0df696f38",
   "metadata": {
    "id": "e384a898-9424-4067-800b-eff0df696f38"
   },
   "outputs": [],
   "source": [
    "frames = []\n",
    "timestamps = []\n",
    "for t, frame in clip.iter_frames(with_times=True):\n",
    "    frames.append(frame)  # This is not recommended especially when a video is large\n",
    "    timestamps.append(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e20bb0f-7dff-49d9-9fb3-711f0e7ce17c",
   "metadata": {
    "id": "8e20bb0f-7dff-49d9-9fb3-711f0e7ce17c"
   },
   "outputs": [],
   "source": [
    "print(\"The total size of recovered frames: {:.2f} GB\".format(len(frames) * np.size(frames[0]) / 1024 / 1024 / 1024), '\\n')\n",
    "print(\"The first 10 timestamps: \", timestamps[:10], '\\n')\n",
    "print(\"Time interval between two frames (sec): \", timestamps[1] - timestamps[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa34724-b597-4783-a5e5-7605d7a8f873",
   "metadata": {
    "id": "faa34724-b597-4783-a5e5-7605d7a8f873"
   },
   "outputs": [],
   "source": [
    "# Draw four randomly selected frames\n",
    "idxs = np.random.randint(0, len(frames), size=4)\n",
    "idxs = sorted(idxs)\n",
    "\n",
    "fig, axes = plt.subplots(figsize=(7, 5), nrows=2, ncols=2)\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    idx = idxs[i]\n",
    "    ax.imshow(frames[idx])\n",
    "    ax.axis('off')\n",
    "    ax.text(10, 30, f't={timestamps[idx]:.2f} sec', fontdict={'color': 'red'})\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6baae6b3-b705-45dd-bea9-6efef7a72627",
   "metadata": {
    "id": "6baae6b3-b705-45dd-bea9-6efef7a72627"
   },
   "source": [
    "---\n",
    "\n",
    "## Transcription: `Distil-Whisper`\n",
    "- HuggingFace's `Distil-Whisper` is six time faster and 49% smaller than OpenAI `Whisper`.\n",
    "- But it supports only English.\n",
    "- Source code: [https://github.com/huggingface/distil-whisper](https://github.com/huggingface/distil-whisper)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb70ac51-c91a-408a-a102-f2b297d39fc9",
   "metadata": {
    "id": "cb70ac51-c91a-408a-a102-f2b297d39fc9"
   },
   "source": [
    "### Extracting the audio fron the video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9caf2c97-22b2-4cb2-8105-1dadeb779a3f",
   "metadata": {
    "id": "9caf2c97-22b2-4cb2-8105-1dadeb779a3f"
   },
   "outputs": [],
   "source": [
    "audio = clip.audio\n",
    "\n",
    "print(\"Duration (sec): \", audio.duration)\n",
    "print(\"Sampling rate(Hz) :  \", audio.fps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9f57bc-8822-4aec-a5e0-c8998ce33410",
   "metadata": {
    "id": "4c9f57bc-8822-4aec-a5e0-c8998ce33410"
   },
   "outputs": [],
   "source": [
    "raw_audio = list(audio.iter_frames())  # Extracting raw audio signal\n",
    "raw_audio = np.array(raw_audio)\n",
    "\n",
    "print(\"The shape of audio signal: \", raw_audio.shape)  # It is a stereo audio\n",
    "mono_audio = raw_audio.mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92468ef-b7da-4ec1-8dc5-9be3baa3a5cf",
   "metadata": {
    "id": "c92468ef-b7da-4ec1-8dc5-9be3baa3a5cf"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4, 3))\n",
    "plt.plot(mono_audio)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9080c6d-299c-415d-94b3-be85c22826b6",
   "metadata": {
    "id": "a9080c6d-299c-415d-94b3-be85c22826b6"
   },
   "source": [
    "### Load `Distil-Whisper` based on the instruction in the [Github repository](https://github.com/huggingface/distil-whisper?tab=readme-ov-file#chunked-long-form)\n",
    "- The code below is to load `Distil-Whisper`. It looks complicated, but it's just the copy-and-pasted codes in the instruction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "PV3DSb5cNghA",
   "metadata": {
    "id": "PV3DSb5cNghA"
   },
   "outputs": [],
   "source": [
    "!pip install transformers==4.49.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13a95d1-547f-4858-8ec1-3a2bd71b32b7",
   "metadata": {
    "id": "b13a95d1-547f-4858-8ec1-3a2bd71b32b7"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "model_id = \"distil-whisper/distil-large-v2\"\n",
    "\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "    model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=model,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    feature_extractor=processor.feature_extractor,\n",
    "    max_new_tokens=128,\n",
    "    chunk_length_s=10,\n",
    "    torch_dtype=torch_dtype,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09265e6-d450-4423-a1b5-82ef04aba47a",
   "metadata": {
    "id": "a09265e6-d450-4423-a1b5-82ef04aba47a"
   },
   "outputs": [],
   "source": [
    "results = pipe({\"raw\": raw_audio.mean(axis=1), 'sampling_rate': audio.fps}, return_timestamps=True)\n",
    "for chunk in results['chunks']:\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56b928a-4d3b-4f8f-b4d6-0b7ea60991f9",
   "metadata": {
    "id": "a56b928a-4d3b-4f8f-b4d6-0b7ea60991f9"
   },
   "source": [
    "---\n",
    "\n",
    "## Visual Annotations using Ultralytics YOLO\n",
    "The YOLO (You Only Look Once) family of models is the most popular choice for real-time computer vision tasks, such as object detection, segmentation, and pose estimation. Many practitioners consider YOLO their \"go-to\" models because it is remarkably fast, lightweight, and easy to use, while still delivering state-of-the-art performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "NJeSO6uWOcK3",
   "metadata": {
    "id": "NJeSO6uWOcK3"
   },
   "outputs": [],
   "source": [
    "!pip install -U ultralytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129d70ec-11ae-4e1e-a843-6c6203c15613",
   "metadata": {
    "id": "129d70ec-11ae-4e1e-a843-6c6203c15613"
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from ultralytics import YOLO\n",
    "\n",
    "frame = frames[0]\n",
    "img = Image.fromarray(frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d33f38c-4f1b-447c-87d2-7805ad31fb74",
   "metadata": {
    "id": "1d33f38c-4f1b-447c-87d2-7805ad31fb74"
   },
   "source": [
    "### Object detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ca5a2e-6132-45a0-8ca4-859cfe373805",
   "metadata": {
    "id": "58ca5a2e-6132-45a0-8ca4-859cfe373805"
   },
   "outputs": [],
   "source": [
    "model = YOLO(\"yolo26n.pt\")  # Load a pretrained model\n",
    "results = model(img)\n",
    "results[0].show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4fe52c3-7c1e-42df-aebc-3c15d863b874",
   "metadata": {
    "id": "d4fe52c3-7c1e-42df-aebc-3c15d863b874"
   },
   "source": [
    "- `results` variable contains the prediction results for a given list images. Since we fed only one image, `results` contains one result.\n",
    "- `result` variable below the prediction result for our image, and it contains all bounding boxes detected in the image. In our case, there are two bounding boxes.\n",
    "- YOLO models are trained on the COCO dataset, which supports the following classes:\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44d520f-e0c0-413e-ada5-9bc077a1061d",
   "metadata": {
    "id": "b44d520f-e0c0-413e-ada5-9bc077a1061d"
   },
   "outputs": [],
   "source": [
    "# Prediction format\n",
    "result = results[0].boxes.cpu().numpy()\n",
    "print(\"Predicted bounding boxes (x, y, w, h) in pixels: \\n\", result.xywh)\n",
    "print(\"\\nPredicted classes: \", result.cls)\n",
    "print(\"\\nSupproted classes: \", results[0].names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e9dbaf-3c0e-4184-a9f6-9dcf1fa97622",
   "metadata": {
    "id": "88e9dbaf-3c0e-4184-a9f6-9dcf1fa97622"
   },
   "source": [
    "### Segmentation\n",
    "Segmentation predicts a set of pixels that outline each object in each bounding box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210fe376-a6e8-433c-a91c-0c36d104dc68",
   "metadata": {
    "id": "210fe376-a6e8-433c-a91c-0c36d104dc68"
   },
   "outputs": [],
   "source": [
    "model = YOLO(\"yolo26n-seg.pt\")\n",
    "results = model(img)\n",
    "results[0].show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143fbcbe-38cc-4223-949b-70a87277b662",
   "metadata": {
    "id": "143fbcbe-38cc-4223-949b-70a87277b662"
   },
   "source": [
    "### Pose estimation\n",
    "Pose estimation predicts the coordinates (in pixels) of the following 17 keypoints:\n",
    "- **Face (5)**: Nose, Left Eye, Right Eye, Left Ear, Right Ear\n",
    "- **Upper body (6)**: Left Shoulder, Right Shoulder, Left Elbow, Right Elbow, Left Wrist, Right Wrist\n",
    "- **Lower body (6)**: Left Hip, Right Hip, Left Knee, Right Knee, Left Ankle, Right Ankle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60136be5-a10d-42e0-93d2-945086d14090",
   "metadata": {
    "id": "60136be5-a10d-42e0-93d2-945086d14090"
   },
   "outputs": [],
   "source": [
    "model = YOLO(\"yolo26n-pose.pt\")\n",
    "results = model(img)\n",
    "results[0].show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e55833f-a031-44f0-a075-0c6f867855fa",
   "metadata": {
    "id": "1e55833f-a031-44f0-a075-0c6f867855fa",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "**Example: Drawing pose estimation for all frames**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259f3ddb-d52b-40d4-9ac3-b474743235dd",
   "metadata": {
    "id": "259f3ddb-d52b-40d4-9ac3-b474743235dd"
   },
   "outputs": [],
   "source": [
    "def func(arr):\n",
    "    result = model([Image.fromarray(arr)], verbose=False)[0]\n",
    "    img = result.plot()\n",
    "    img = img[:, :, [2, 1, 0]]  # BGR 2 RGB\n",
    "\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774eb35b-7b7a-4856-9e83-04a24582d80e",
   "metadata": {
    "id": "774eb35b-7b7a-4856-9e83-04a24582d80e"
   },
   "outputs": [],
   "source": [
    "sub_clip = clip.subclip(t_start=0, t_end=10.0)\n",
    "new_clip = sub_clip.fl_image(func)\n",
    "new_clip.ipython_display(width=360, rd_kwargs={'logger': None})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60043015-b774-4924-b8f8-3eceb345aac9",
   "metadata": {
    "id": "60043015-b774-4924-b8f8-3eceb345aac9"
   },
   "source": [
    "### Gaze detection using L2-CS Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4434414a-0079-4d28-9993-e3807690c2d3",
   "metadata": {
    "id": "4434414a-0079-4d28-9993-e3807690c2d3"
   },
   "outputs": [],
   "source": [
    "!wget 'https://drive.usercontent.google.com/download?id=18S956r4jnHtSeT8z8t3z8AoJZjVnNqPJ&export=download&authuser=1&confirm=t' -O L2CSNet_gaze360.pkl\n",
    "!pip install git+https://github.com/edavalosanaya/L2CS-Net.git@main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaccf310-397b-401a-8c1b-6b4846225d08",
   "metadata": {
    "id": "eaccf310-397b-401a-8c1b-6b4846225d08"
   },
   "outputs": [],
   "source": [
    "from l2cs import Pipeline, render\n",
    "\n",
    "gaze_pipeline = Pipeline(\n",
    "    './L2CSNet_gaze360.pkl',\n",
    "    arch='ResNet50',\n",
    "    device=torch.device('cuda') # or 'gpu'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf425fb-cf62-4f8b-b531-8e8a23a6a43e",
   "metadata": {
    "id": "2bf425fb-cf62-4f8b-b531-8e8a23a6a43e"
   },
   "outputs": [],
   "source": [
    "results = gaze_pipeline.step(frame)\n",
    "frame = render(np.copy(frame), results)\n",
    "\n",
    "plt.figure(figsize=(4, 3))\n",
    "plt.imshow(frame)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d2afd9-5270-4fda-932b-5fefc0f6e7cc",
   "metadata": {
    "id": "24d2afd9-5270-4fda-932b-5fefc0f6e7cc"
   },
   "outputs": [],
   "source": [
    "def func(arr):\n",
    "    result = gaze_pipeline.step(arr)\n",
    "    img = render(np.copy(arr), result)\n",
    "\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99e7673-ef8c-4bbe-9ad4-f1f1ad19a6c1",
   "metadata": {
    "id": "b99e7673-ef8c-4bbe-9ad4-f1f1ad19a6c1"
   },
   "outputs": [],
   "source": [
    "sub_clip = clip.subclip(t_start=0, t_end=10.0)\n",
    "new_clip = sub_clip.fl_image(func)\n",
    "new_clip.ipython_display(maxduration=60.1, width=360, rd_kwargs={'logger': None})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e486cfc3-b7bb-446e-b765-d6c62b62dcaa",
   "metadata": {
    "id": "e486cfc3-b7bb-446e-b765-d6c62b62dcaa"
   },
   "outputs": [],
   "source": [
    "clip.close()\n",
    "new_clip.close()\n",
    "sub_clip.close()\n",
    "audio.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440408be-011b-48c4-9ec6-fdea15b8d8c4",
   "metadata": {
    "id": "440408be-011b-48c4-9ec6-fdea15b8d8c4"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
